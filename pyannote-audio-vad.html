<!DOCTYPE html SYSTEM "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <title>
            pyannote.audio Voice Activity Detection
        </title>
    </head>
    <body>
        <center><h3>pyannote.audio Voice Activity Detection</h3></center>

        <p>This is a local ELAN recognizer that offers a voice activity
        detection (VAD) service based on
        <a href="https://github.com/pyannote/pyannote-audio">pyannote.audio</a>,
        an open-source Python toolkit that separates audio into speech vs.
        non-speech (among other tasks).  The user is able to select an input
        audio file linked to this transcript, which this recognizer then passes
        on to pyannote.audio for processing, producing a new tier containing
        the segments of speech that pyannote.audio has identified.
        </p>

        <p>By default, this recognizer uses the pre-trained pyannote.audio
        pipelines for voice activity detection made available through
        <a href="https://huggingface.co/pyannote/">Hugging Face</a>.  Since
        these models are gated, it is necessary to sign into Hugging Face and
        request access to the following two models:</p>

        <ul>
            <li><a href="https://huggingface.co/pyannote/segmentation-3.0">pyannote/segmentation-3.0</a>
            <li><a href="https://huggingface.co/pyannote/voice-activity-detection">pyannote/voice-activity-detection</a>
        </ul>

        <p>Once access has been granted to these repositories, it is possible
        to create a <a href="https://hf.co/settings/token">Hugging Face access
        token</a> to provide to this recognizer, allowing it to load the
        corresponding models directly from Hugging Face.</p>

        <p>In addition to using these &quot;out-of-the-box&quot;, pretrained
        pipelines, it is also possible to provide a user-defined checkpoint
        (.ckpt) representing a fine-tuned pyannote.audio segmentation model,
        which will then be used instead of pyannote.audio's provided
        segmentation model.</p>
    </body>
</html>
